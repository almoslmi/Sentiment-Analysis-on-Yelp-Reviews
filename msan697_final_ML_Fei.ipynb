{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data = spark.read.json(\"review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|      date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2016-07-12|    0|VfBHSwC5Vz_pbFluy...|    5|My girlfriend and...|     0|cjpdDjZyprfyDG3Rl...|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Business average rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check distribution of the rating star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "|    5|1988003|\n",
      "|    1| 639849|\n",
      "|    3| 570819|\n",
      "|    2| 402396|\n",
      "|    4|1135830|\n",
      "+-----+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print review_data.groupBy(review_data[\"stars\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude neutral review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_neg(star):\n",
    "    if star <3:\n",
    "        return int(0) #negative\n",
    "    elif star >3 :\n",
    "        return int(1) #positive\n",
    "    else:\n",
    "        return int(2) #neutral\n",
    "    \n",
    "star_to_senti = udf(lambda x:pos_neg(x))\n",
    "train_test_DF_raw = review_data.select('text',star_to_senti('stars').alias('label')).filter(\"label != 2\") #exclude neutral reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "train_test_DF = train_test_DF_raw.withColumn(\"label\", train_test_DF_raw[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    0|1042245|\n",
      "|    1|3123833|\n",
      "+-----+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print train_test_DF.groupBy(train_test_DF[\"label\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|My girlfriend and...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_DF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|My girlfriend and I stayed here for nights and loved it The location of this hotel and very decent price makes this an amazing deal When you walk out the front door Scott Monument and Princes street are right in front of you Edinburgh Castle and the Royal Mile is a minute walk via a close right around the corner and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible The hotel itself was also very nice with a reasonably priced bar very considerate staff and small but comfortable rooms with excellent bathrooms and showers Only two minor complaints are no telephones in room for room service not a huge deal for us and no AC in the room but they have huge windows which can be fully opened The staff were incredible though letting us borrow umbrellas for the rain giving us maps and directions and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free I would highly recommend this hotel to friends and when I return to Edinburgh which I most definitely will I will be staying here without any hesitation|1.0  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_num_punct(text):\n",
    "\n",
    "    my_string = text.replace(\"-\", \" \")\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", my_string)  # delete stuff but leave at least a space to avoid clumping together\n",
    "\n",
    "    nopunct = nopunct.split()\n",
    "    #nopunct = [stemmer.stem(w).strip(\" \") for w in nopunct] #remove stop word and normalize word using stemmer.\n",
    "    nopunct = [w.strip() for w in nopunct]\n",
    "    nopunct = ' '.join(nopunct)\n",
    "    \n",
    "    return nopunct\n",
    "\n",
    "udf_num_punct = udf(lambda x:remove_num_punct(x))\n",
    "review_rmsw = train_test_DF.select(udf_num_punct('text').alias('text'), 'label')\n",
    "review_rmsw.show(1,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### setNumFeatures(20)\n",
    "n_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "hashingTF = HashingTF().setNumFeatures(n_features).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set= review_rmsw.randomSplit([0.8, 0.2])\n",
    "train_set = train_set.cache()\n",
    "test_set = test_set.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute accuracy on the test set \n",
    "def evaluate_metric(predictions):\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "    print \"Area under ROC curve:\",evaluator.evaluate(predictions)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(\"F1_score = %0.4f\" %(f1))\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy = %0.4f\" %(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 ms, sys: 3.09 ms, total: 6.4 ms\n",
      "Wall time: 8.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr =  LogisticRegression(maxIter=100, regParam=0.01, elasticNetParam=0.8)\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])\n",
    "logreg_model=pipeline.fit(train_set)\n",
    "predictions = logreg_model.transform(test_set)\n",
    "#print evaluation metrics\n",
    "evaluate_metric(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print \"Logistic regression features column=\",lr.getFeaturesCol()\n",
    "# print \"logistic regression label column=\",lr.getLabelCol()\n",
    "# print \"Logistic regression threshold=\",lr.getThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print \"Tokenizer:\"\n",
    "# print tokenizer.explainParams()\n",
    "# print \"***************************\"\n",
    "# print \"Remover:\"\n",
    "# print remover.explainParams()\n",
    "# print \"***************************\"\n",
    "# print \"HashingTF:\"\n",
    "# print hashingTF.explainParams()\n",
    "# print \"***************************\"\n",
    "# print \"IDF:\"\n",
    "# print idf.explainParams()\n",
    "# print \"***************************\"\n",
    "# print \"LogisticRegression:\"\n",
    "# print lr.explainParams()\n",
    "# print \"***************************\"\n",
    "# print \"Pipeline:\"\n",
    "# print pipeline.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation to find best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(hashingTF.numFeatures,[100,1000,10000])\\\n",
    "#     .addGrid(idf.minDocFreq,[0,10,100])\\\n",
    "#     .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "# cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cvModel = cv.fit(train_set)\n",
    "# print \"Area under the ROC curve for best fitted model =\",evaluator.evaluate(cvModel.transform(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print \"Area under ROC curve for non-tuned model:\",evaluator.evaluate(predictions)\n",
    "# print \"Area under ROC curve for fitted model:\",evaluator.evaluate(cvModel.transform(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Unigram Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nb = NaiveBayes(smoothing = 1.0, modelType = \"multinomial\")\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, nb])\n",
    "nb_model=pipeline.fit(train_set)\n",
    "nb_prediction = nb_model.transform(test_set)\n",
    "#print evaluation metrics\n",
    "evaluate_metric(nb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|label|               words|            filtered|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|A Brazilian culin...|  1.0|[a, brazilian, cu...|[brazilian, culin...|(20,[2,5,6,7,8,9,...|(20,[2,5,6,7,8,9,...|[-23.283075401701...|[0.24375668975552...|       1.0|\n",
      "|A C wasn t workin...|  1.0|[a, c, wasn, t, w...|[c, wasn, working...|(20,[0,2,3,6,7,8,...|(20,[0,2,3,6,7,8,...|[-24.680653558029...|[0.22196539433659...|       1.0|\n",
      "|A Cut above the r...|  1.0|[a, cut, above, t...|[cut, rest, impre...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-153.87506614939...|[0.21145996055961...|       1.0|\n",
      "|A First class and...|  1.0|[a, first, class,...|[first, class, hi...|(20,[0,1,3,4,5,7,...|(20,[0,1,3,4,5,7,...|[-23.483276229103...|[0.20768851984696...|       1.0|\n",
      "|A Foodie Delight ...|  1.0|[a, foodie, delig...|[foodie, delight,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-39.744091278362...|[0.23102879917604...|       1.0|\n",
      "|A GREAT PLACE I a...|  1.0|[a, great, place,...|[great, place, ca...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-38.920670426532...|[0.24050654803659...|       1.0|\n",
      "|A GRIM AND FROSTB...|  1.0|[a, grim, and, fr...|[grim, frostbitte...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-65.193651146981...|[0.22201572920279...|       1.0|\n",
      "|A Good Dentist Ma...|  1.0|[a, good, dentist...|[good, dentist, m...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-61.051170447921...|[0.25826904479970...|       1.0|\n",
      "|A Great PLACE We ...|  1.0|[a, great, place,...|[great, place, fo...|(20,[0,1,2,4,9,10...|(20,[0,1,2,4,9,10...|[-14.473555768499...|[0.24218790496901...|       1.0|\n",
      "|A Kaufmann s trad...|  1.0|[a, kaufmann, s, ...|[kaufmann, tradit...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-41.949305586463...|[0.28939343679403...|       1.0|\n",
      "|A Kent OH classic...|  1.0|[a, kent, oh, cla...|[kent, oh, classi...|(20,[0,3,5,6,8,9,...|(20,[0,3,5,6,8,9,...|[-15.011199175741...|[0.23876078524337...|       1.0|\n",
      "|A Las Vegas must ...|  1.0|[a, las, vegas, m...|[las, vegas, must...|(20,[1,2,3,4,7,9,...|(20,[1,2,3,4,7,9,...|[-11.402151735256...|[0.25845061579203...|       1.0|\n",
      "|A MA ZING This is...|  1.0|[a, ma, zing, thi...|[ma, zing, favori...|(20,[0,1,2,3,5,6,...|(20,[0,1,2,3,5,6,...|[-18.558021040290...|[0.23438126278518...|       1.0|\n",
      "|A MAZ ING we had ...|  1.0|[a, maz, ing, we,...|[maz, ing, severa...|(20,[2,4,6,7,10,1...|(20,[2,4,6,7,10,1...|[-11.431240556365...|[0.27674840166983...|       1.0|\n",
      "|A Perfect Fit Nan...|  1.0|[a, perfect, fit,...|[perfect, fit, na...|(20,[1,2,3,4,5,6,...|(20,[1,2,3,4,5,6,...|[-28.040354690576...|[0.26573068010479...|       1.0|\n",
      "|A Perfect Fit is ...|  1.0|[a, perfect, fit,...|[perfect, fit, be...|(20,[0,1,2,3,5,6,...|(20,[0,1,2,3,5,6,...|[-29.128334193799...|[0.22107051247162...|       1.0|\n",
      "|A Pittsburgh clas...|  1.0|[a, pittsburgh, c...|[pittsburgh, clas...|(20,[3,6,8,12,13,...|(20,[3,6,8,12,13,...|[-7.9921879750971...|[0.25499451472409...|       1.0|\n",
      "|A Toronto institu...|  1.0|[a, toronto, inst...|[toronto, institu...|(20,[0,1,3,4,6,7,...|(20,[0,1,3,4,6,7,...|[-17.518888567053...|[0.24593737969301...|       1.0|\n",
      "|A Unexpected Gem ...|  1.0|[a, unexpected, g...|[unexpected, gem,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-65.434814007752...|[0.28849773931194...|       1.0|\n",
      "|A arrets de murra...|  1.0|[a, arrets, de, m...|[arrets, de, murr...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-26.555038598795...|[0.27369334301566...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_prediction.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bigram Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "#remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "bigram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
    "hashingTF_bigram = HashingTF().setNumFeatures(n_features).setInputCol(\"bigrams\").setOutputCol(\"rawFeatures\")\n",
    "idf_bigram = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nb = NaiveBayes(smoothing = 1.0, modelType = \"multinomial\")\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,bigram,hashingTF_bigram,idf_bigram, nb])\n",
    "nb_model_bigram=pipeline.fit(train_set)\n",
    "nb_prediction_bigram = nb_model_bigram.transform(test_set)\n",
    "\n",
    "#print evaluation metrics\n",
    "evaluate_metric(nb_prediction_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_prediction_bigram.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Trigram Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tribgram tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "#remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "trigram = NGram(n=3, inputCol=\"filtered\", outputCol=\"trigrams\")\n",
    "hashingTF_trigram = HashingTF().setNumFeatures(n_features).setInputCol(\"trigrams\").setOutputCol(\"rawFeatures\")\n",
    "idf_trigram = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nb = NaiveBayes(smoothing = 1.0, modelType = \"multinomial\")\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,trigram,hashingTF_trigram,idf_trigram, nb])\n",
    "nb_model_trigram=pipeline.fit(train_set)\n",
    "nb_prediction_trigram = nb_model_trigram.transform(test_set)\n",
    "#print evaluation metrics\n",
    "evaluate_metric(nb_prediction_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_prediction_trigram.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "%%time\n",
    "rf = RandomForestClassifier(maxDepth=20)\n",
    "\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, nb])\n",
    "rf_model = pipeline.fit(train_set)\n",
    "rf_prediction = rf_model.transform(test_set)\n",
    "#print evaluation metrics\n",
    "evaluate_metric(rf_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 6: Multilayer perceptron classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify layers for the neural network:\n",
    "# input layer of size 20 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "\n",
    "#%%time\n",
    "\n",
    "layers = [n_features, 5 , 2] \n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=128, seed=1234)\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, trainer])\n",
    "nn_model = pipeline.fit(train_set)\n",
    "\n",
    "nn_prediction = nn_model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " +--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|label|               words|            filtered|         rawFeatures|            features|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|A Brazilian culin...|  1.0|[a, brazilian, cu...|[brazilian, culin...|(20,[2,5,6,7,8,9,...|(20,[2,5,6,7,8,9,...|       1.0|\n",
      "|A C wasn t workin...|  1.0|[a, c, wasn, t, w...|[c, wasn, working...|(20,[0,2,3,6,7,8,...|(20,[0,2,3,6,7,8,...|       1.0|\n",
      "|A Cut above the r...|  1.0|[a, cut, above, t...|[cut, rest, impre...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|       1.0|\n",
      "|A First class and...|  1.0|[a, first, class,...|[first, class, hi...|(20,[0,1,3,4,5,7,...|(20,[0,1,3,4,5,7,...|       1.0|\n",
      "|A Foodie Delight ...|  1.0|[a, foodie, delig...|[foodie, delight,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print nn_prediction.groupBy(nn_prediction[\"label\"], nn_prediction[\"prediction\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   # evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "   # print \"Area under ROC curve:\",evaluator.evaluate(nn_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " F1_score = 0.6418\n"
     ]
    }
   ],
   "source": [
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(nn_prediction)\n",
    "    print(\"F1_score = %0.4f\" %(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7492\n"
     ]
    }
   ],
   "source": [
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(nn_prediction)\n",
    "    print(\"Accuracy = %0.4f\" %(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print evaluation metrics\n",
    "#evaluate_metric(nn_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
